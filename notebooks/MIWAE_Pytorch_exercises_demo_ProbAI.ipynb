{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vjdYRutQxukL"
      },
      "source": [
        "# Deep learning meets missing data: Doing it MIWAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gMq1ifa_uUna"
      },
      "source": [
        "In this notebook, we'll show how to learn a deep generative model on a small and **incomplete** continuous data set. We will also show how to **impute** the missing values of this data set. \n",
        "\n",
        "This is based on the following paper, available [on arXiv](https://arxiv.org/abs/1812.02633):\n",
        "\n",
        "P.-A. Mattei & J. Frellsen, **MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets**, *Proceedings of the 36th International Conference on Machine Learning*, PMLR 97:4413-4423, 2019.\n",
        "\n",
        "It is possible to run this notebook in Google Colab, which allows to benefit from free GPU computing.\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "    <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/pamattei/MIWAE_Pytorch_exercises_demo_ProbAI.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dawQVMWrvxYu"
      },
      "source": [
        "# Installing and loading useful stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "74ZkYf2YTn0s",
        "outputId": "32af9c46-79a2-4e33-e4fa-75977d062dc3"
      },
      "outputs": [],
      "source": [
        "!pip3 install --user --upgrade scikit-learn # We need to update it to run missForest\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.io\n",
        "import scipy.sparse\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.distributions as td\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_SLCK6HRT7C2"
      },
      "outputs": [],
      "source": [
        "def mse(xhat,xtrue,mask): # MSE function for imputations\n",
        "    xhat = np.array(xhat)\n",
        "    xtrue = np.array(xtrue)\n",
        "    return np.mean(np.power(xhat-xtrue,2)[~mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "f0HjKh4WyB-Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0RpNVgHjuPzC"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DeBa5C1WXoGN"
      },
      "source": [
        "We'll use the Iris data set from scikit-learn:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RIQ9W-Uv_ur0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WTtS1FJQjw3P"
      },
      "source": [
        "It is also possible to use the breast cancer or the Boston data sets by uncommenting one of the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4RtDGEQjWNW1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()['data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0qZQA7peYNI8"
      },
      "outputs": [],
      "source": [
        "#from sklearn.datasets import load_boston\n",
        "#data = load_boston(True)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxWks15H_GOB"
      },
      "source": [
        "It is also possible to use the \"white wine\" or \"red wine\" UCI data sets by uncommenting one of the following cells:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8ImVc9R52qdg"
      },
      "outputs": [],
      "source": [
        "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "#data = np.array(pd.read_csv(url, low_memory=False, sep=';'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "akd4tc0m-HMz"
      },
      "outputs": [],
      "source": [
        "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
        "#data = np.array(pd.read_csv(url, low_memory=False, sep=';'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OiX9pkXkhG9Q"
      },
      "outputs": [],
      "source": [
        "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
        "#data = np.array(pd.read_csv(url, low_memory=False, sep=','))[:,0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Rp_Xbte_2zVK"
      },
      "outputs": [],
      "source": [
        "xfull = (data - np.mean(data,0))/np.std(data,0)\n",
        "n = xfull.shape[0] # number of observations\n",
        "p = xfull.shape[1] # number of features\n",
        "xfull = np.random.permutation(xfull)  # Randomly permute the data\n",
        "\n",
        "mnar_size = int(0.5 * n)  # 10% of the data will be MNAR\n",
        "mcar_size = int(0.25 * n)  # 10% of the data will be MCAR\n",
        "test_size = n - mnar_size - mcar_size  # Remaining data for testing\n",
        "data_mnar, data_mcar, data_test = data[:mnar_size], data[mnar_size:mnar_size+mcar_size], data[mnar_size+mcar_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "np.random.seed(1234)\n",
        "n,p = data_mcar.shape\n",
        "perc_miss = 0.5 # 50% of missing data\n",
        "data_mcar_nan = np.copy(data_mcar)\n",
        "mask_mcar = np.random.rand(n) < perc_miss # create a mask for missing data\n",
        "mask_mcar = np.concatenate([mask_mcar.reshape(-1, 1),\n",
        "                            np.ones((data_mcar.shape[0],data.shape[1]))]\n",
        "                            ,axis=1) # duplicate the mask for all features\n",
        "for i in range(data_mcar.shape[0]):\n",
        "    if not mask_mcar[i,0]: # if the first column is missing\n",
        "        data_mcar_nan[i,0] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MNAR Pattern\n",
        "np.random.seed(1234)\n",
        "full_mean, full_std = np.mean(data,0), np.std(data,0)\n",
        "missing_mnar = data_mnar[:,0]> full_mean[0] + 0.01 * full_std[0] # missing pattern based on the first feature\n",
        "data_mnar_nan = np.copy(data_mnar)\n",
        "mask = np.ones_like(data_mnar, dtype=bool) # initialize mask with all True\n",
        "for i in range(data_mnar.shape[0]):\n",
        "    if missing_mnar[i]: # if the condition is met, set the corresponding row to False\n",
        "        mask[i,0] = False\n",
        "for i in range(data_mnar.shape[0 ]):\n",
        "    if missing_mnar[i]:  # randomly set some features to be missing\n",
        "        data_mnar_nan[i,0] = np.nan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xhat_0 = np.copy(data_mnar_nan)\n",
        "xhat_0[np.isnan(data_mnar_nan)] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "im4zpQifyI9S"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IqK000IKVW4L"
      },
      "outputs": [],
      "source": [
        "h = 128 # number of hidden units in (same for all MLPs)\n",
        "d = 1 # dimension of the latent space\n",
        "K = 20 # number of IS during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rwcaqZ5qyNid"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "srUk6d53ZCJT"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zepRWE3kDI_-"
      },
      "source": [
        "We will use a **deep latent variable model with a Gaussian prior and a Student's t observation model**. This can be written:\n",
        "\n",
        "$$p(\\mathbf{x}_1,...,\\mathbf{x}_n) = \\prod_{i=1}^n p(\\mathbf{x}_i|\\mathbf{z}_i)p(\\mathbf{z}_i),$$\n",
        "$$p(\\mathbf{z}_i) = \\mathcal{N}(\\mathbf{z}_i|\\mathbf{0}_d,\\mathbf{I}_d), $$\n",
        "$$p(\\mathbf{x}_i|\\mathbf{z}_i) = \\text{St} (\\mathbf{x}_i|\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i),\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i),\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i)),$$\n",
        "\n",
        "where $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$, $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathcal{S}_p^{++}$, and $\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}_+^p$ are functions parametrised by deep neural nets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQGrEj5flTeK"
      },
      "source": [
        "The weights of these nets are stored in a parameter $\\boldsymbol{\\theta}$. We choose to use the following simple architecture, where the 3 neural nets share the first layers:\n",
        "$$f_{\\boldsymbol{\\theta}} (\\mathbf{z})=\\sigma(\\mathbf{W}_1\\sigma(\\mathbf{W}_0\\mathbf{z}+\\mathbf{b}_0)+\\mathbf{b}_1) $$\n",
        "\n",
        "$$\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\mathbf{W}_\\boldsymbol{\\mu}f_{\\boldsymbol{\\theta}} (\\mathbf{z})+\\mathbf{b}_\\boldsymbol{\\mu}, $$\n",
        "\n",
        "$$\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\text{Diag}\\left(\\text{Softplus}(\\mathbf{W}_\\boldsymbol{\\sigma}f_{\\boldsymbol{\\theta}} (\\mathbf{z})+\\mathbf{b}_\\boldsymbol{\\sigma}) + 10^{-3}\\right), $$\n",
        "\n",
        "$$\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z}) = \\text{Softplus}(\\mathbf{W}_\\boldsymbol{\\nu}f_{\\boldsymbol{\\theta}} (\\mathbf{z})+\\mathbf{b}_\\boldsymbol{\\nu}) + 3. $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B8Tw89qNmLte"
      },
      "source": [
        "A few **non-essential remarks** about this architecture:\n",
        "\n",
        "* This parametrisation is quite close to the one we use in the MIWAE paper. The main difference is that we use $\\sigma = \\text{ReLU}$ (which leads to faster training) while we used $\\sigma = \\text{tanh}$ in the paper.\n",
        "*   We use a [location-scale parametrisation](https://en.wikipedia.org/wiki/Location%E2%80%93scale_family) of the t distribution, following [the parametrisation available in TensorFlow](https://www.tensorflow.org/api_docs/python/tf/distributions/StudentT). Note in particular that $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})$ is not the covariance matrix of $\\mathbf{x} | \\mathbf{z}$. When it exitsts, the actual covariance matrix is diagonal with diagonal  $$ \\frac{\\text{diag}(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}} (\\mathbf{z}))^2 \\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z})}{\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}(\\mathbf{z})-2}$$ (where all operations are made entrywise).\n",
        "*   The fact that the covariance matrix is diagonal means that we assume that **the features are independent conditionnally on the latent variable** (which is customary for DLVMs).\n",
        "* We add $3$ to the neural net that outputs the degrees of freedom. This is to guarantee that the tails of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} | \\mathbf{z})$ are not too heavy. Indeed, having too heavy tails might imply that the mean of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} | \\mathbf{z})$ does not exist! Adding 3 implies that the degrees of freedom is always larger than 3, implying in turn that **at least the first 3 moments of $p_{\\boldsymbol{\\theta}}(\\mathbf{x} | \\mathbf{z})$ are well-defined.**\n",
        "* We add $10^{-3}$ to the diagonal entries of $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}$ to prevent singularities, [as advocated in our NeurIPS 2018 paper](https://papers.nips.cc/paper/7642-leveraging-the-exact-likelihood-of-deep-latent-variable-models). Why $10^{-3}$ specifically? Because, since the data have unit variance, this will imply that the latent variable explains at most $99.9999\\%$ of the variance of the data, which does not seem too restrictive. **This choice might be poor if the data are not standardised.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yh_slL43JnLR"
      },
      "source": [
        "We begin with the prior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check CUDA availability and device info\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wTzTVsO7XZTB"
      },
      "outputs": [],
      "source": [
        "p_z = td.Independent(td.Normal(loc=torch.zeros(d, device=device), scale=torch.ones(d, device=device)), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1KBnFeeMJ5S6"
      },
      "source": [
        " Now, we define the **decoder**, which will be the backbone of the three functions $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^p$, $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathcal{S}_p^{++}$, and $\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}_+^p$. Here, the output space of this decoder is $\\mathbb{R}^{3p}$. Some additional operations are needed for $\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathcal{S}_p^{++}$, and $\\boldsymbol{\\nu}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow \\mathbb{R}_+^p$, but it'll be more convenient to implement them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XiCPYSprUtyA"
      },
      "outputs": [],
      "source": [
        "decoder = nn.Sequential(\n",
        "    torch.nn.Linear(d, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, 3*p),  # the decoder will output both the mean, the scale, and the number of degrees of freedoms (hence the 3*p)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1MgZJM8Yy8qG"
      },
      "source": [
        "# Posterior approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yHrpYUZGzCf2"
      },
      "source": [
        "We will build a Gaussian posterior approximation $q(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N} (\\mathbf{z}|\\mathbf{m}_\\gamma (\\mathbf{x}),\\mathbf{S}_\\gamma (\\mathbf{x}))$ by using an **encoder** that mimicks the architecture of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7kKPm4eiV3dl"
      },
      "outputs": [],
      "source": [
        "encoder = nn.Sequential(\n",
        "    torch.nn.Linear(p, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(h, 2*d),  # the encoder will output both the mean and the diagonal covariance\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "CxZQogI52Koa",
        "outputId": "7accaff7-c109-47d7-ac59-fcc46f110afc"
      },
      "outputs": [],
      "source": [
        "encoder.to(device) # we'll use the GPU\n",
        "decoder.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zYzqx4syz2eT"
      },
      "source": [
        "# Building the MIWAE loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7TWrlHSU82Zj"
      },
      "source": [
        "We will define a function that, given the imputation $\\iota(\\mathbf{x}^\\text{o})$ and the mask, computes the MIWAE bound."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lnkox7-Q8hkA"
      },
      "source": [
        "$$\n",
        "\\mathcal{L}_K (\\boldsymbol{\\theta,\\gamma}) = \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{z}_{i1},\\ldots,\\mathbf{z}_{iK} \\sim q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}^\\text{o}_i)} \\left[ \\log\\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\boldsymbol{\\theta}}(\\mathbf{x}_i^\\text{o}|\\mathbf{z}_{ik})p(\\mathbf{z}_{ik})}{q_{\\boldsymbol{\\gamma}}(\\mathbf{z}_{ik}|\\mathbf{x}^\\text{o}_i)} \\right].\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "P13f2MiGUXi3"
      },
      "outputs": [],
      "source": [
        "def miwae_loss(iota_x,mask):\n",
        "  batch_size = iota_x.shape[0]\n",
        "  out_encoder = encoder(iota_x)\n",
        "  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
        "  \n",
        "  zgivenx = q_zgivenxobs.rsample([K])\n",
        "  zgivenx_flat = zgivenx.reshape([K*batch_size,d])\n",
        "  \n",
        "  out_decoder = decoder(zgivenx_flat)\n",
        "  all_means_obs_model = out_decoder[..., :p]\n",
        "  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
        "  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
        "  \n",
        "  data_flat = torch.Tensor.repeat(iota_x,[K,1]).reshape([-1,1])\n",
        "  tiledmask = torch.Tensor.repeat(mask,[K,1])\n",
        "  \n",
        "  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
        "  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([K*batch_size,p])\n",
        "  \n",
        "  logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([K,batch_size])\n",
        "  logpz = p_z.log_prob(zgivenx)\n",
        "  logq = q_zgivenxobs.log_prob(zgivenx)\n",
        "  \n",
        "  neg_bound = -torch.mean(torch.logsumexp(logpxobsgivenz + logpz - logq,0))\n",
        "  \n",
        "  return neg_bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8zl7ue5rDo1J"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oUWloElrESwP"
      },
      "source": [
        "# Single imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E-QpI8yN_aQl"
      },
      "source": [
        "We can do single imputation using self normalised IS:\n",
        "\\begin{equation*}\n",
        "\\mathbb E [\\mathbf{x}^{\\textrm{m}} | \\mathbf{x}^{\\textrm{o}}] \\approx \\sum_{l=1}^L w_l \\, \\mathbf{x}^{\\textrm{m}}_{(l)},\n",
        "\\end{equation*}\n",
        "where $(\\mathbf{x}^{\\textrm{m}}_{(1)},\\mathbf{z}_{(1)}),\\ldots,(\\mathbf{x}^{\\textrm{m}}_{(L)},\\mathbf{z}_{(L)})$ are i.i.d.~samples from $p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{\\textrm{m}}|\\mathbf{x}^{\\textrm{o}},\\mathbf{z})q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}^{\\textrm{o}})$ and \n",
        "\\begin{equation*}\n",
        "w_l=\\frac{r_l}{r_1+\\ldots+r_L}, \\; \\textrm{with} \\; r_l = \\frac{p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{\\textrm{o}}|\\mathbf{z}_{(l)})p(\\mathbf{z}_{(l)})}{q_{\\boldsymbol{\\gamma}}(\\mathbf{z}_{(l)}|\\mathbf{x}^{\\textrm{o}})}.\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6ihzUc3iF731"
      },
      "outputs": [],
      "source": [
        "def miwae_impute(iota_x,mask,L, device,):\n",
        "  batch_size = iota_x.shape[0]\n",
        "  out_encoder = encoder(iota_x)\n",
        "  q_zgivenxobs = td.Independent(td.Normal(loc=out_encoder[..., :d],scale=torch.nn.Softplus()(out_encoder[..., d:(2*d)])),1)\n",
        "  \n",
        "  zgivenx = q_zgivenxobs.rsample([L])\n",
        "  zgivenx_flat = zgivenx.reshape([L*batch_size,d])\n",
        "  \n",
        "  out_decoder = decoder(zgivenx_flat)\n",
        "  all_means_obs_model = out_decoder[..., :p]\n",
        "  all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
        "  all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
        "  \n",
        "  data_flat = torch.Tensor.repeat(iota_x,[L,1]).reshape([-1,1]).to(device)\n",
        "  tiledmask = torch.Tensor.repeat(mask,[L,1]).to(device)\n",
        "  \n",
        "  all_log_pxgivenz_flat = torch.distributions.StudentT(loc=all_means_obs_model.reshape([-1,1]),scale=all_scales_obs_model.reshape([-1,1]),df=all_degfreedom_obs_model.reshape([-1,1])).log_prob(data_flat)\n",
        "  all_log_pxgivenz = all_log_pxgivenz_flat.reshape([L*batch_size,p])\n",
        "  \n",
        "  logpxobsgivenz = torch.sum(all_log_pxgivenz*tiledmask,1).reshape([L,batch_size])\n",
        "  logpz = p_z.log_prob(zgivenx)\n",
        "  logq = q_zgivenxobs.log_prob(zgivenx)\n",
        "  \n",
        "  xgivenz = td.Independent(td.StudentT(loc=all_means_obs_model, scale=all_scales_obs_model, df=all_degfreedom_obs_model),1)\n",
        "\n",
        "  imp_weights = torch.nn.functional.softmax(logpxobsgivenz + logpz - logq,0) # these are w_1,....,w_L for all observations in the batch\n",
        "  xms = xgivenz.sample().reshape([L,batch_size,p])\n",
        "  xm=torch.einsum('ki,kij->ij', imp_weights, xms) \n",
        "  \n",
        "\n",
        "  \n",
        "  return xm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dODsWjNkBW_5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ROKruvy7FVP9"
      },
      "outputs": [],
      "source": [
        "def weights_init(layer):\n",
        "  if type(layer) == nn.Linear: torch.nn.init.orthogonal_(layer.weight)\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1445
        },
        "colab_type": "code",
        "id": "sBn0XinLC4Uy",
        "outputId": "9a8b6bf7-84c4-48c5-ba2c-91d278f2b807"
      },
      "outputs": [],
      "source": [
        "miwae_loss_train=np.array([])\n",
        "mse_train=np.array([])\n",
        "mse_train2=np.array([])\n",
        "bs = 64 # batch size\n",
        "n_epochs = 2002\n",
        "xhat = np.copy(xhat_0) # This will be out imputed data matrix\n",
        "\n",
        "encoder.apply(weights_init)\n",
        "decoder.apply(weights_init)\n",
        "\n",
        "for ep in range(1,n_epochs):\n",
        "  perm = np.random.permutation(n) # We use the \"random reshuffling\" version of SGD\n",
        "  if n>bs:\n",
        "    batches_data = np.array_split(xhat_0[perm,], n/bs)\n",
        "    batches_mask = np.array_split(mask[perm,], n/bs)\n",
        "  else :\n",
        "    batches_data = [xhat_0[perm,]]\n",
        "    batches_mask = [mask[perm,]]\n",
        "  for it in range(len(batches_data)):\n",
        "    optimizer.zero_grad()\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    b_data = torch.from_numpy(batches_data[it]).float().to(device)\n",
        "    b_mask = torch.from_numpy(batches_mask[it]).float().to(device)\n",
        "    loss = miwae_loss(iota_x = b_data,mask = b_mask)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if ep % 100 == 1:\n",
        "    print('Epoch %g' %ep)\n",
        "    print('MIWAE likelihood bound  %g' %(-np.log(K)-miwae_loss(iota_x = torch.from_numpy(xhat_0).float().to(device),mask = torch.from_numpy(mask).float().to(device)).cpu().data.numpy())) # Gradient step      \n",
        "    \n",
        "    ### Now we do the imputation\n",
        "    \n",
        "    xhat[~mask] = miwae_impute(\n",
        "                               iota_x = torch.from_numpy(xhat_0).float().to(device),\n",
        "                               mask = torch.from_numpy(mask).float().to(device),\n",
        "                               L=10,\n",
        "                               device=device,\n",
        "                               ).cpu().data.numpy()[~mask]\n",
        "    err = np.array([mse(xhat,data_mnar,mask)])\n",
        "    mse_train = np.append(mse_train,err,axis=0)\n",
        "    print('Imputation MSE  %g' %err)\n",
        "    print('-----')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XZsYAmymH28c"
      },
      "source": [
        "# Comparisons with other methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84kWm6OVIRq8"
      },
      "source": [
        "We make use of the recent [IterativeImputer](https://scikit-learn.org/dev/auto_examples/impute/plot_iterative_imputer_variants_comparison.html) mehod implemented in scikit-learn. It allows, in particular, to use an imputation technique quite similar to the popular missForest algorithm of  [Stekhoven & Bühlmann (2011)](https://academic.oup.com/bioinformatics/article/28/1/112/219101)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "XQMQZzoqEXxw",
        "outputId": "1d088f73-e981-4204-822d-cdeededc06ac"
      },
      "outputs": [],
      "source": [
        "missforest = IterativeImputer(max_iter=20, estimator=ExtraTreesRegressor(n_estimators=100))\n",
        "iterative_ridge = IterativeImputer(max_iter=20, estimator=BayesianRidge())\n",
        "missforest.fit(data_mnar_nan)\n",
        "iterative_ridge.fit(data_mnar_nan)\n",
        "xhat_mf = missforest.transform(data_mnar_nan)\n",
        "xhat_ridge = iterative_ridge.transform(data_mnar_nan)\n",
        "mean_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "mean_imp.fit(data_mnar_nan)\n",
        "xhat_mean = mean_imp.transform(data_mnar_nan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "colab_type": "code",
        "id": "PF3VgTEGEXu-",
        "outputId": "2d9bbffb-8f95-4081-9b5f-6757a556851f"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1,n_epochs,100),mse_train,color=\"blue\")\n",
        "plt.axhline(y=mse(xhat_mf,data_mnar,mask),  linestyle='-',color=\"red\")\n",
        "plt.axhline(y=mse(xhat_ridge,data_mnar,mask),  linestyle='-',color=\"orange\")\n",
        "plt.axhline(y=mse(xhat_mean,data_mnar,mask),  linestyle='-',color=\"green\")\n",
        "plt.legend([\"MIWAE\",\"missForest\",\"Iterative ridge\", \"Mean imputation\"])\n",
        "plt.title(\"Imputation MSE\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ACoF-oZACqBi"
      },
      "source": [
        "# Exercise:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mMCyAOVmCr11"
      },
      "source": [
        "Try to come up with a better single imputation estimate, and implement it!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MIWAE_Pytorch_exercises_demo_ProbAI.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "hackathon0403",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
