{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Deep learning meets missing data: Doing it MIWAE on MAR MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this notebook, we'll learn a deep generative model on the MAR‚Äêmasked MNIST dataset and impute its missing pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Installing and loading useful stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --user --upgrade scikit-learn\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions as td\n",
    "from torch import optim\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Loading MNIST and applying MAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "mnist = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "data = mnist.data.float().view(-1, 784).numpy() / 255.0 + np.random.normal(0, 1, mnist.data.float().view(-1, 784).numpy().shape) * 0.1\n",
    "\n",
    "data, _ = train_test_split(\n",
    "    data, train_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "bias, unbias = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(f\"Bias data shape: {bias.shape}\")\n",
    "print(f\"Unbias data shape: {unbias.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mar_mask(data):\n",
    "    masks = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for i, example in enumerate(data):\n",
    "        h = (1. / (784. / 2.)) * np.sum(example[392:]) + 0.3\n",
    "        pi = np.random.binomial(2, h)\n",
    "        _mask = np.ones(example.shape[0])\n",
    "        if pi == 0:\n",
    "            _mask[196:392] = 0\n",
    "        elif pi == 1:\n",
    "            _mask[:392] = 0\n",
    "        elif pi == 2:\n",
    "            _mask[:196] = 0\n",
    "        masks[i, :] = _mask\n",
    "    return masks\n",
    "\n",
    "def create_mnar_mask(data):\n",
    "    masks = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for i, example in enumerate(data):\n",
    "        h = (1. / (784. / 2.)) * np.sum(example[:392]) + 0.3\n",
    "        pi = np.random.binomial(2, h)\n",
    "        _mask = np.ones(example.shape[0])\n",
    "        if pi == 0:\n",
    "            _mask[196:392] = 0\n",
    "        elif pi == 1:\n",
    "            _mask[:392] = 0\n",
    "        elif pi == 2:\n",
    "            _mask[:196] = 0\n",
    "        masks[i, :] = _mask\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "mask_bias = create_mnar_mask(bias)\n",
    "data_obs_bias = bias.copy()\n",
    "# data_obs_bias = (data_obs_bias - data_obs_bias.mean()) / data_obs_bias.std()\n",
    "data_obs_bias[mask_bias == 0] = np.nan\n",
    "\n",
    "mask_unbias = create_mnar_mask(unbias)\n",
    "data_obs_unbias = unbias.copy()\n",
    "# data_obs_unbias = (data_obs_unbias - data_obs_unbias.mean()) / data_obs_unbias.std()\n",
    "data_obs_unbias[mask_unbias == 0] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# %%\n",
    "n = bias.shape[0]\n",
    "indices = np.random.choice(n, 15, replace=False)\n",
    "plt.figure(figsize=(30, 4))\n",
    "for i, idx in enumerate(indices):\n",
    "    orig = bias[idx].reshape(28, 28)\n",
    "    masked_bias = data_obs_bias[idx].copy()\n",
    "    masked_bias[mask_bias[idx] == 0] = 0.5  # Use grey (0.5) for masked pixels\n",
    "    plt.subplot(2, 15, i + 1)\n",
    "    plt.imshow(orig, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.ylabel('Original')\n",
    "    plt.subplot(2, 15, i + 16)\n",
    "    plt.imshow(masked_bias.reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.ylabel('Masked')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfull_bias = data_obs_bias.copy()\n",
    "n, p = xfull_bias.shape\n",
    "xobs_zero_bias = np.nan_to_num(xfull_bias, 0)\n",
    "mask_bool_bias = mask_bias.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example of xfull with NaNs colored red\n",
    "plt.figure(figsize=(8, 6))\n",
    "example_idx = 0\n",
    "example_data = xfull_bias[example_idx].reshape(28, 28)\n",
    "\n",
    "# Create a custom colormap where NaNs are red\n",
    "import matplotlib.colors as colors\n",
    "cmap = plt.cm.gray.copy()\n",
    "cmap.set_bad(color='red')\n",
    "\n",
    "plt.imshow(example_data, cmap=cmap, vmin=0, vmax=1)\n",
    "plt.title(f'Example {example_idx}: Original data with NaNs (red)')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 256\n",
    "d = 1\n",
    "K = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "p_z = td.Independent(td.Normal(torch.zeros(d).to(device), torch.ones(d).to(device)), 1)\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(d, h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(h, h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(h, 3 * p),\n",
    ")\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(p, h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(h, h),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(h, 2 * d),\n",
    ")\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MIWAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_loss(iota_x, mask):\n",
    "    batch = iota_x.shape[0]\n",
    "    out = encoder(iota_x)\n",
    "    q = td.Independent(td.Normal(out[..., :d], torch.nn.Softplus()(out[..., d:])), 1)\n",
    "    z = q.rsample([K]).reshape([K * batch, d])\n",
    "    dec = decoder(z)\n",
    "    mu, scale, df = dec[:, :p], torch.nn.Softplus()(dec[:, p:2*p]) + 1e-3, torch.nn.Softplus()(dec[:, 2*p:]) + 3\n",
    "    scale = torch.full_like(scale, 0.1)  # Ensure scale is not too small\n",
    "\n",
    "    data_flat = iota_x.repeat(K, 1).reshape(-1, 1)\n",
    "    mask_flat = mask.repeat(K, 1)\n",
    "    # log_px = td.StudentT(df=df.reshape(-1,1), loc=mu.reshape(-1,1), scale=scale.reshape(-1,1)).log_prob(data_flat)\n",
    "    log_px = td.Normal(loc=mu.reshape(-1, 1), scale=scale.reshape(-1, 1)).log_prob(data_flat)\n",
    "    log_px = log_px.reshape(K * batch, p)\n",
    "    # plt.imshow(mask_flat.reshape(K, batch, 28,28)[0][0].cpu().numpy(), cmap='gray')\n",
    "    # plt.show()\n",
    "    log_px_obs = (log_px * mask_flat).reshape(K, batch, p).sum(-1)\n",
    "    log_pz = p_z.log_prob(z.reshape(K, batch, d))\n",
    "    log_q = q.log_prob(z.reshape(K, batch, d))\n",
    "    bound = torch.logsumexp(log_px_obs + log_pz - log_q, 0) - torch.log(torch.tensor(K, dtype=torch.float, device=iota_x.device))\n",
    "    return -bound.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Single imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_impute(iota_x, mask, L, device):\n",
    "    batch = iota_x.shape[0]\n",
    "    out = encoder(iota_x)\n",
    "    q = td.Independent(td.Normal(out[..., :d], torch.nn.Softplus()(out[..., d:])), 1)\n",
    "    z = q.rsample([L]).reshape([L * batch, d])\n",
    "    dec = decoder(z)\n",
    "    mu, scale, df = dec[:, :p], torch.nn.Softplus()(dec[:, p:2*p]) + 1e-3, torch.nn.Softplus()(dec[:, 2*p:]) + 3\n",
    "    scale = torch.full_like(scale, 0.1)  # Ensure scale is not too small\n",
    "    # scale = 0.01\n",
    "    # log_px = td.StudentT(df=df.reshape(-1,1), loc=mu.reshape(-1,1), scale=scale.reshape(-1,1)).log_prob(iota_x.repeat(L,1).reshape(-1,1)).reshape(L, batch, p)\n",
    "    log_px = td.Normal(loc=mu.reshape(-1, 1), scale=scale.reshape(-1, 1)).log_prob(iota_x.repeat(L, 1).reshape(-1, 1)).reshape(L, batch, p)\n",
    "\n",
    "    log_pz = p_z.log_prob(z.reshape(L, batch, d))\n",
    "    log_q = q.log_prob(z.reshape(L, batch, d))\n",
    "    w = torch.nn.functional.softmax(log_px.sum(-1) + log_pz - log_q, 0)\n",
    "    x_samples = td.Independent(td.StudentT(df=df.reshape(-1,1), loc=mu.reshape(-1,1), scale=scale.reshape(-1,1)),1).sample().reshape(L, batch, p)\n",
    "    return torch.einsum('lb,lbp->bp', w, x_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhat_bias = xobs_zero_bias.copy()\n",
    "mask_bias_t = mask_bool_bias.astype(float)\n",
    "bs = 64\n",
    "epochs = 20\n",
    "\n",
    "for ep in range(0, epochs):\n",
    "    if ep % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            total_bound = -miwae_loss(torch.tensor(xhat_bias, dtype=torch.float).to(device), torch.tensor(mask_bias_t, dtype=torch.float).to(device))\n",
    "            print(f'Epoch {ep} bound {total_bound.item()}')\n",
    "            xhat_bias_tensor = miwae_impute(torch.tensor(xhat_bias, dtype=torch.float).to(device), torch.tensor(mask_bias_t, dtype=torch.float).to(device), 10, device).cpu().numpy()\n",
    "            print(f'Imputation MSE {np.mean((xhat_bias_tensor - bias)[mask_bool_bias == 0]**2)}')\n",
    "            fig, axs = plt.subplots(4, 4, figsize=(20, 8))\n",
    "            axs[0,0].set_title('Bias')\n",
    "            axs[0,1].set_title('Masked')\n",
    "            axs[0,2].set_title('Just MIWAE')\n",
    "            axs[0,3].set_title('MIWAE + Bias')\n",
    "            for i in range(4):                \n",
    "                axs[i,0].imshow(bias[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "                axs[i,1].imshow(xobs_zero_bias[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "                axs[i,1].set_title('Masked')\n",
    "                axs[i,2].imshow(xhat_bias_tensor[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "                axs[i,2].set_title('Just MIWAE')\n",
    "                axs[i,3].imshow(xhat_bias_tensor[i].reshape(28, 28) * (1-mask_bool_bias[i]).reshape(28, 28) + bias[i].reshape(28, 28) * mask_bool_bias[0].reshape(28, 28)\n",
    "                            , cmap='gray', vmin=0, vmax=1)\n",
    "            plt.show()\n",
    "    idx = np.random.permutation(n)\n",
    "    for i in range(0, n, bs):\n",
    "        batch_id = idx[i:i+bs]\n",
    "        b_x = torch.tensor(xhat_bias[batch_id], dtype=torch.float).to(device)\n",
    "        b_m = torch.tensor(mask_bias_t[batch_id], dtype=torch.float).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = miwae_loss(b_x, b_m)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBM Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class EBM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super(EBM, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "ebm = EBM(input_dim=unbias.shape[1], hidden_dim=32).to(device)\n",
    "ebm_optimizer = optim.Adam(ebm.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwae_sample(N_samples, device):\n",
    "    \"\"\"\n",
    "    Sample L imputations from the MIWAE model for missing values in iota_x.\n",
    "    Returns: samples of shape (L, batch_size, p)\n",
    "    \"\"\"\n",
    "    z = td.Normal(loc=torch.zeros(N_samples, d, device=device), scale=torch.ones(N_samples, d, device=device)).sample()\n",
    "    z =  z.reshape([N_samples, -1, d])  # Reshape to (L, batch_size, d)\n",
    "\n",
    "    out_decoder = decoder(z)\n",
    "    all_means_obs_model = out_decoder[..., :p]\n",
    "    all_scales_obs_model = torch.nn.Softplus()(out_decoder[..., p:(2*p)]) + 0.001\n",
    "    all_degfreedom_obs_model = torch.nn.Softplus()(out_decoder[..., (2*p):(3*p)]) + 3\n",
    "\n",
    "    xgivenz = td.Independent(\n",
    "        td.Normal(\n",
    "            loc=all_means_obs_model,\n",
    "            scale=0.1,\n",
    "            # df=all_degfreedom_obs_model\n",
    "        ), 1\n",
    "    )\n",
    "    x_samples = xgivenz.sample()\n",
    "    return x_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfull_unbias = data_obs_unbias.copy()\n",
    "n, p = xfull_unbias.shape\n",
    "xobs_unbias_zero = np.nan_to_num(xfull_unbias, 0)\n",
    "mask_unbias_bool = mask_unbias.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miwaebm_impute(iota_x, mask, L, device):\n",
    "    \n",
    "    batch = iota_x.shape[0]\n",
    "    out = encoder(iota_x)\n",
    "    q = td.Independent(td.Normal(out[..., :d], torch.nn.Softplus()(out[..., d:])), 1)\n",
    "    z = q.rsample([L]).reshape([L * batch, d])\n",
    "    dec = decoder(z)\n",
    "    mu, scale, df = dec[:, :p], torch.nn.Softplus()(dec[:, p:2*p]) + 1e-3, torch.nn.Softplus()(dec[:, 2*p:]) + 3\n",
    "    scale = torch.full_like(scale, 0.1)  # Ensure scale is not too small\n",
    "    # scale = 0.01\n",
    "    # log_px = td.StudentT(df=df.reshape(-1,1), loc=mu.reshape(-1,1), scale=scale.reshape(-1,1)).log_prob(iota_x.repeat(L,1).reshape(-1,1)).reshape(L, batch, p)\n",
    "    log_px = td.Normal(loc=mu.reshape(-1, 1), scale=scale.reshape(-1, 1)).log_prob(iota_x.repeat(L, 1).reshape(-1, 1)).reshape(L, batch, p).sum(-1)\n",
    "    energy_px = ebm(iota_x.repeat(L,1).reshape(-1, p)).reshape(L, batch)\n",
    "    log_px_corrected = log_px - energy_px\n",
    "    log_pz = p_z.log_prob(z.reshape(L, batch, d))\n",
    "    log_q = q.log_prob(z.reshape(L, batch, d))\n",
    "    w = torch.nn.functional.softmax(log_px_corrected + log_pz - log_q, 0)\n",
    "    x_samples = td.Independent(td.StudentT(df=df.reshape(-1,1), loc=mu.reshape(-1,1), scale=scale.reshape(-1,1)),1).sample().reshape(L, batch, p)\n",
    "    return torch.einsum('lb,lbp->bp', w, x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Prepare TensorDataset and DataLoader for batching\n",
    "xhat_unbias = np.copy(xfull_unbias)\n",
    "xhat_unbias[np.isnan(data_obs_unbias)] = 0\n",
    "xhat_unbias_0_tensor = torch.from_numpy(xhat_unbias).float().to(device)\n",
    "dataset = TensorDataset(xhat_unbias_0_tensor)\n",
    "loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "miwae_loss_train = np.array([])\n",
    "mse_train = np.array([])\n",
    "\n",
    "for param in decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for ep in range(1, n_epochs):\n",
    "\n",
    "    # if ep % 1 == 1:\n",
    "    \n",
    "    for b_data in loader:\n",
    "        b_data = b_data[0].to(device)\n",
    "        ebm_optimizer.zero_grad()\n",
    "\n",
    "        energy_gt = ebm(b_data)\n",
    "\n",
    "        # Sample from MIWAE\n",
    "        x_samples = miwae_sample(N_samples=b_data.shape[0], device=device).squeeze()\n",
    "        energy_miwae = ebm(x_samples)\n",
    "\n",
    "        loss = torch.mean(energy_gt) - torch.mean(energy_miwae)\n",
    "        reg_loss = torch.mean(energy_gt**2) + torch.mean(energy_miwae**2)\n",
    "        interp = torch.rand(b_data.shape[0], 1, device=device)\n",
    "        x_interp = interp * b_data + (1 - interp) * x_samples\n",
    "        x_interp.requires_grad_(True)\n",
    "        energy_interp = torch.mean(ebm(x_interp))\n",
    "        grad_interp = torch.autograd.grad(\n",
    "            outputs=energy_interp,\n",
    "            inputs=x_interp,\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "\n",
    "        grad_reg_loss = grad_interp.norm(2, dim=1) \n",
    "        loss += 0.1 * reg_loss + 0.1 * grad_reg_loss.mean()\n",
    "        loss.backward()\n",
    "        ebm_optimizer.step()\n",
    "    if True :\n",
    "        print(f'Epoch {ep}')\n",
    "        print(f'EBM loss: {loss.item()}, EBM gt : {torch.mean(energy_gt).item()}, EBM MIWAE: {torch.mean(energy_miwae).item()}')\n",
    "        # Fix shape mismatch: mask_mcar is shape (n, p), but may contain extra columns due to concatenation\n",
    "        # TODO: imputation MSE calc\n",
    "        print('-----')\n",
    "        print(f'Epoch {ep} bound {total_bound.item()}')\n",
    "        xhat_bias_tensor = miwaebm_impute(torch.tensor(xhat_bias, dtype=torch.float).to(device), torch.tensor(mask_bias, dtype=torch.float).to(device), 10, device).cpu().detach().numpy()\n",
    "        print(f'Imputation MSE {np.mean((xhat_bias_tensor - bias)[mask_bool_bias == 0]**2)}')\n",
    "        fig, axs = plt.subplots(4, 4, figsize=(20, 8))\n",
    "        axs[0,0].set_title('Bias')\n",
    "        axs[0,1].set_title('Masked')\n",
    "        axs[0,2].set_title('Just MIWAE')\n",
    "        axs[0,3].set_title('MIWAE + Bias')\n",
    "        for i in range(4):                \n",
    "            axs[i,0].imshow(unbias[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "            axs[i,1].imshow(xobs_zero_bias[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "            axs[i,1].set_title('Masked')\n",
    "            axs[i,2].imshow(xhat_bias_tensor[i].reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "            axs[i,2].set_title('Just MIWAE')\n",
    "            axs[i,3].imshow(xhat_bias_tensor[i].reshape(28, 28) * (1-mask_bool_bias[i]).reshape(28, 28) + bias[i].reshape(28, 28) * mask_bool_bias[0].reshape(28, 28)\n",
    "                        , cmap='gray', vmin=0, vmax=1)\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon0403",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
