{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributions as td\n",
    "from torch import optim\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier on MNIST digits dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MNIST(root='./', train=True, download=True, transform=transforms.ToTensor())\n",
    "dataset_test = MNIST(root='./', train=False, download=True, transform=transforms.ToTensor())\n",
    "dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "def train_model(model, dataloader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "model = SimpleNN()\n",
    "train_model(model, dataloader, epochs=5)\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test set: {accuracy:.2f}%')\n",
    "evaluate_model(model, dataloader_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MNIST(root='./', train=True, download=True, transform=transforms.ToTensor())\n",
    "x_data, y_data = dataset.data, dataset.targets\n",
    "\n",
    "mask = (y_data <= 4)\n",
    "x_data, y_data = x_data[mask], y_data[mask]\n",
    "\n",
    "proportions = {0: 0.12, 1: 0.33, 2: 0.28, 3: 0.20, 4: 0.07}\n",
    "num_samples = int(0.6 * len(y_data))\n",
    "counts = {cls: int(proportions[cls] * num_samples) for cls in proportions}\n",
    "\n",
    "bias_idx = []\n",
    "remaining_idx = []\n",
    "\n",
    "for cls in proportions:\n",
    "    cls_idx = (y_data == cls).nonzero(as_tuple=True)[0]\n",
    "    cls_idx = cls_idx[torch.randperm(len(cls_idx))]\n",
    "    bias_cls_idx = cls_idx[:counts[cls]]\n",
    "    remain_cls_idx = cls_idx[counts[cls]:]\n",
    "    \n",
    "    bias_idx.extend(bias_cls_idx.tolist())\n",
    "    remaining_idx.extend(remain_cls_idx.tolist())\n",
    "\n",
    "remaining_y = y_data[remaining_idx]\n",
    "# min_class_count = min([(remaining_y == cls).sum().item() for cls in proportions])\n",
    "min_class_count = 10\n",
    "\n",
    "unbias_idx = []\n",
    "for cls in proportions:\n",
    "    cls_remaining_idx = (remaining_y == cls).nonzero(as_tuple=True)[0]\n",
    "    cls_remaining_idx = cls_remaining_idx[torch.randperm(len(cls_remaining_idx))][:min_class_count]\n",
    "    unbias_idx.extend([remaining_idx[i] for i in cls_remaining_idx])\n",
    "\n",
    "bias = x_data[bias_idx]\n",
    "bias_y = y_data[bias_idx]\n",
    "unbias = x_data[unbias_idx]\n",
    "unbias_y = y_data[unbias_idx]\n",
    "\n",
    "# Random subsampling to ensure equal class distribution in unbias set\n",
    "# unbias = unbias[]\n",
    "# unbias_y = unbias_y[]\n",
    "\n",
    "bias.shape, bias_y.shape, unbias.shape, unbias_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms in the same figure with two panels\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot histogram for bias_y\n",
    "ax1.hist(bias_y, bins=np.arange(6) - 0.5)\n",
    "ax1.set_xlabel('Label')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Histogram of bias_y labels')\n",
    "ax1.set_xticks(range(5))\n",
    "\n",
    "# Plot histogram for unbias_y\n",
    "ax2.hist(unbias_y, bins=np.arange(6) - 0.5)\n",
    "ax2.set_xlabel('Label')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Histogram of unbias_y labels')\n",
    "ax2.set_xticks(range(5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256, 2)\n",
    "        self.fc_logvar = nn.Linear(256, 2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 256),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z).view(-1, 1, 28, 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "vae_init = VAE()\n",
    "vae_init.to(device).eval()\n",
    "\n",
    "dataset = TensorDataset(bias.float().unsqueeze(1) / 255., bias_y)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "latents, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        mu, _ = vae_init.encode(x.to(device))\n",
    "        latents.append(mu.cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "z = torch.cat(latents).numpy()\n",
    "lbl = torch.cat(labels).numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=lbl, s=4)\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Training data latent space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a VAE on biased MNIST digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "vae_biased = VAE()\n",
    "vae_biased.load_state_dict(vae_init.state_dict())\n",
    "vae_biased.to(device).eval()\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "bias_norm = bias.float().unsqueeze(1) / 255.\n",
    "full_dataset = TensorDataset(bias_norm)\n",
    "\n",
    "train_len = int(0.9 * len(full_dataset))\n",
    "val_len = len(full_dataset) - train_len\n",
    "train_set, val_set = random_split(full_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(vae_biased.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "epochs = 100\n",
    "\n",
    "epoch_iter = tqdm(range(epochs), desc=\"Training\")\n",
    "for epoch in epoch_iter:\n",
    "    vae_biased.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = vae_biased(x)\n",
    "        loss = loss_fn(recon_x, x, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    vae_biased.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(device)\n",
    "            recon_x, mu, logvar = vae_biased(x)\n",
    "            loss = loss_fn(recon_x, x, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    epoch_iter.set_postfix(train_loss=train_loss / len(train_loader.dataset),\n",
    "                           val_loss=val_loss / len(val_loader.dataset))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = vae_biased.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "vae_biased.load_state_dict(best_state)\n",
    "vae_biased.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(bias.float().unsqueeze(1) / 255., bias_y)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "latents, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        mu, _ = vae_biased.encode(x.to(device))\n",
    "        latents.append(mu.cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "z = torch.cat(latents).numpy()\n",
    "lbl = torch.cat(labels).numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=lbl, s=4)\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Training data latent space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_images = vae_biased.decode(torch.randn(10000, 2).to(device)).cpu().detach()\n",
    "\n",
    "# Plot the generated images\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        idx = i * 10 + j\n",
    "        axes[i, j].imshow(generated_images[idx].squeeze(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram of the resulting samples :\n",
    "class_decoded = model(generated_images.view(-1, 28 * 28))\n",
    "class_decoded = class_decoded.argmax(dim=1).numpy()\n",
    "plt.hist(class_decoded, bins=np.arange(11) - 0.5, density=True)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Generated Samples from EBM')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE on the unbiased MNIST dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "vae_unbiased = VAE()\n",
    "vae_unbiased.load_state_dict(vae_init.state_dict())\n",
    "vae_unbiased.to(device).eval()\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "unbias_norm = unbias.float().unsqueeze(1) / 255.\n",
    "full_dataset = TensorDataset(unbias_norm)\n",
    "\n",
    "train_len = int(0.9 * len(full_dataset))\n",
    "val_len = len(full_dataset) - train_len\n",
    "train_set, val_set = random_split(full_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(vae_unbiased.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "epochs = 100\n",
    "\n",
    "epoch_iter = tqdm(range(epochs), desc=\"Training\")\n",
    "for epoch in epoch_iter:\n",
    "    vae_unbiased.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = vae_unbiased(x)\n",
    "        loss = loss_fn(recon_x, x, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    vae_unbiased.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(device)\n",
    "            recon_x, mu, logvar = vae_unbiased(x)\n",
    "            loss = loss_fn(recon_x, x, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    epoch_iter.set_postfix(train_loss=train_loss / len(train_loader.dataset),\n",
    "                           val_loss=val_loss / len(val_loader.dataset))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = vae_unbiased.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "vae_unbiased.load_state_dict(best_state)\n",
    "vae_unbiased.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(bias.float().unsqueeze(1) / 255., bias_y)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "latents, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        mu, _ = vae_unbiased.encode(x.to(device))\n",
    "        latents.append(mu.cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "z = torch.cat(latents).numpy()\n",
    "lbl = torch.cat(labels).numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=lbl, s=4)\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Training data latent space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_images = vae_unbiased.decode(torch.randn(10000, 2).to(device)).cpu().detach()\n",
    "\n",
    "# Plot the generated images\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        idx = i * 10 + j\n",
    "        axes[i, j].imshow(generated_images[idx].squeeze(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the histogram of the resulting samples :\n",
    "class_decoded = model(generated_images.view(-1, 28 * 28))\n",
    "class_decoded = class_decoded.argmax(dim=1).numpy()\n",
    "plt.hist(class_decoded, bins=np.arange(11) - 0.5, density=True)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Generated Samples from EBM')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "vae_finetuing = VAE()\n",
    "vae_finetuing.load_state_dict(vae_biased.state_dict())\n",
    "vae_finetuing.to(device).eval()\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "unbias_norm = unbias.float().unsqueeze(1) / 255.\n",
    "full_dataset = TensorDataset(unbias_norm)\n",
    "\n",
    "train_len = int(0.9 * len(full_dataset))\n",
    "val_len = len(full_dataset) - train_len\n",
    "train_set, val_set = random_split(full_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(vae_finetuing.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "epochs = 100\n",
    "\n",
    "epoch_iter = tqdm(range(epochs), desc=\"Training\")\n",
    "for epoch in epoch_iter:\n",
    "    vae_finetuing.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = vae_finetuing(x)\n",
    "        loss = loss_fn(recon_x, x, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    vae_finetuing.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch[0].to(device)\n",
    "            recon_x, mu, logvar = vae_finetuing(x)\n",
    "            loss = loss_fn(recon_x, x, mu, logvar)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    epoch_iter.set_postfix(train_loss=train_loss / len(train_loader.dataset),\n",
    "                           val_loss=val_loss / len(val_loader.dataset))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state = vae_finetuing.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "vae_finetuing.load_state_dict(best_state)\n",
    "vae_finetuing.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(bias.float().unsqueeze(1) / 255., bias_y)\n",
    "loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "latents, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        mu, _ = vae_finetuing.encode(x.to(device))\n",
    "        latents.append(mu.cpu())\n",
    "        labels.append(y)\n",
    "\n",
    "z = torch.cat(latents).numpy()\n",
    "lbl = torch.cat(labels).numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=lbl, s=4)\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Training data latent space')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_images = vae_finetuing.decode(torch.randn(10000, 2).to(device)).cpu().detach()\n",
    "\n",
    "# Plot the generated images\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        idx = i * 10 + j\n",
    "        axes[i, j].imshow(generated_images[idx].squeeze(), cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram of the resulting samples :\n",
    "class_decoded = model(generated_images.view(-1, 28 * 28))\n",
    "class_decoded = class_decoded.argmax(dim=1).numpy()\n",
    "plt.hist(class_decoded, bins=np.arange(11) - 0.5, density=True)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Generated Samples from EBM')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an EBM on the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unbias_dataset = TensorDataset(unbias.float().unsqueeze(1) / 255., unbias_y)\n",
    "unbias_loader = DataLoader(unbias_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "latents_unbias, images_unbias, labels_unbias = [], [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in unbias_loader:\n",
    "        mu, _ = vae_biased.encode(x.to(device))\n",
    "        latents_unbias.append(mu.cpu())\n",
    "        images_unbias.append(x.cpu())\n",
    "        labels_unbias.append(y.cpu())\n",
    "\n",
    "latents_unbias = torch.cat(latents_unbias)\n",
    "images_unbias = torch.cat(images_unbias)\n",
    "labels_unbias = torch.cat(labels_unbias)\n",
    "\n",
    "unbias_tensor_dataset = TensorDataset(latents_unbias, images_unbias, labels_unbias)\n",
    "\n",
    "train_len = int(0.9 * len(unbias_tensor_dataset))\n",
    "val_len = len(unbias_tensor_dataset) - train_len\n",
    "train_ebm_set, val_ebm_set = random_split(unbias_tensor_dataset, [train_len, val_len])\n",
    "\n",
    "train_ebm_loader = DataLoader(train_ebm_set, batch_size=128, shuffle=True)\n",
    "val_ebm_loader = DataLoader(val_ebm_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Energy-Based Model Implementation ######\n",
    "class SmallEBM(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=256):\n",
    "        super(SmallEBM, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "\n",
    "    def energy(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "# # Initialize EBM\n",
    "# ebm = SmallEBM().to(device)\n",
    "# ebm_optimizer = optim.Adam(ebm.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Energy-Based Model Training on Unbiased Latents ######\n",
    "from tqdm import trange\n",
    "\n",
    "ebm_epochs = 50\n",
    "ebm = SmallEBM().to(device)\n",
    "ebm_optimizer = optim.Adam(ebm.parameters(), lr=1e-3)\n",
    "\n",
    "pbar = trange(ebm_epochs, desc=\"EBM Training\")\n",
    "for epoch in pbar:\n",
    "    for latents, _, _ in train_ebm_loader:          # (z, x, y)\n",
    "        batch = latents.to(device)\n",
    "\n",
    "        # Positive energy\n",
    "        pos_energy = ebm.energy(batch)\n",
    "\n",
    "        # Negative samples ~ N(0, I)\n",
    "        neg_samples = torch.randn_like(batch).to(device)\n",
    "        neg_energy = ebm.energy(neg_samples)\n",
    "\n",
    "        # Contrastive term\n",
    "        loss = pos_energy.mean() - neg_energy.mean()\n",
    "\n",
    "        # Energy norm regularization\n",
    "        reg_loss = (pos_energy**2).mean() + (neg_energy**2).mean()\n",
    "\n",
    "        # Gradient regularization\n",
    "        interp = torch.rand(batch.size(0), 1, device=device)\n",
    "        x_interp = (interp * batch + (1 - interp) * neg_samples).requires_grad_(True)\n",
    "        energy_interp = ebm.energy(x_interp).mean()\n",
    "        grad_interp = torch.autograd.grad(energy_interp, x_interp, create_graph=True)[0]\n",
    "        grad_reg_loss = grad_interp.norm(2, dim=1).mean()\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = loss + 0.1 * reg_loss + 0.1 * grad_reg_loss\n",
    "\n",
    "        ebm_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        ebm_optimizer.step()\n",
    "\n",
    "    pbar.set_postfix(loss=total_loss.item(),\n",
    "                     pos_energy=pos_energy.mean().item(),\n",
    "                     neg_energy=neg_energy.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Energy-weighted sampling from the standard normal prior ######\n",
    "n_prior = 100000\n",
    "with torch.no_grad():\n",
    "    prior_samples = torch.randn(n_prior, 2).to(device)\n",
    "    energies = ebm.energy(prior_samples)             # shape (n_prior,)\n",
    "    weights = torch.softmax(-energies, dim=0)        # Boltzmann weights\n",
    "    index_samples = torch.distributions.Categorical(weights).sample((1000,))\n",
    "    ew_priors = prior_samples[index_samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoded = vae_biased.decode(ew_priors.to(device)).cpu().view(-1, 28, 28)\n",
    "\n",
    "fig, axes = plt.subplots(6, 6, figsize=(6, 6), gridspec_kw={'wspace': 0.05, 'hspace': 0.05})\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        axes[i, j].imshow(decoded[i * 6 + j], cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.suptitle('Generated Images from Energy-Weighted Prior Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram of the resulting samples :\n",
    "class_decoded = model(decoded.view(-1, 28 * 28))\n",
    "class_decoded = class_decoded.argmax(dim=1).numpy()\n",
    "plt.hist(class_decoded, bins=np.arange(11) - 0.5, density=True)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Generated Samples from EBM')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=lbl, cmap='tab10', s=4, alpha=0.6)\n",
    "# plt.scatter(prior_samples[:, 0].cpu(), prior_samples[:, 1].cpu(), c='lightgray', s=8, alpha=0.4, label='Prior samples')\n",
    "plt.scatter(ew_priors[:, 0].cpu(), ew_priors[:, 1].cpu(), c='navy', s=12, marker='x', label='Energy-weighted priors')\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Latent Space with Energy-Weighted Priors')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_res = 200\n",
    "x_min, x_max = z[:, 0].min() - 1, z[:, 0].max() + 1\n",
    "y_min, y_max = z[:, 1].min() - 1, z[:, 1].max() + 1\n",
    "xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, grid_res),\n",
    "                        torch.linspace(y_min, y_max, grid_res), indexing='xy')\n",
    "grid = torch.stack([xx.flatten(), yy.flatten()], dim=1).to(device)\n",
    "with torch.no_grad():\n",
    "    energy = ebm.energy(grid).cpu().view(grid_res, grid_res) + torch.distributions.Normal(0, 1).log_prob(grid).sum(dim=1).cpu().view(grid_res, grid_res)\n",
    "plt.figure(figsize=(7, 6))\n",
    "contour = plt.contourf(xx.cpu(), yy.cpu(), energy, levels=50, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(contour, label='Energy')\n",
    "# plt.scatter(z[:, 0], z[:, 1], c=lbl, cmap='tab10', s=4, alpha=0.6)\n",
    "plt.scatter(ew_priors[:, 0].cpu(), ew_priors[:, 1].cpu(), c='navy', s=12, marker='x')\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Latent Space with EBM Energy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing SGLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do SGLD on the resulting ebm:\n",
    "\n",
    "def sgld_step(ebm, x, step_size=0.01, noise_std=0.1):\n",
    "    x.requires_grad_(True)\n",
    "    energy = -ebm.energy(x) + torch.distributions.Normal(0, 1).log_prob(x).sum(dim=1)\n",
    "    grad = torch.autograd.grad(energy.sum(), x)[0]\n",
    "    noise = torch.randn_like(x) * noise_std\n",
    "    x_new = x + step_size * grad + noise\n",
    "    return x_new.detach()\n",
    "\n",
    "def sample_from_ebm(ebm, initial_samples, num_steps=1000, step_size=0.01, noise_std=0.1):\n",
    "    samples = initial_samples.clone().detach()\n",
    "    paths = [samples.clone().detach()]\n",
    "    for i in range(num_steps):\n",
    "        samples = sgld_step(ebm, samples, step_size, noise_std)\n",
    "        # if i % 100 == 0:\n",
    "        paths.append(samples.clone().detach())\n",
    "    return paths\n",
    "\n",
    "\n",
    "paths = sample_from_ebm(ebm, torch.distributions.Normal(0, 1).sample((64, 2)).to(device), num_steps=1000, step_size=0.01, noise_std=0.15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the path by \n",
    "plt.figure(figsize=(7, 6))\n",
    "contour = plt.contourf(xx.cpu(), yy.cpu(), energy, levels=50, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(contour, label='Energy')\n",
    "# plt.scatter(ew_priors[:, 0].cpu(), ew_priors[:, 1].cpu(), c='navy', s=12, marker='x', label='Energy-weighted priors')\n",
    "\n",
    "for path in paths:\n",
    "    plt.scatter(path[0, 0].cpu(), path[0, 1].cpu(), color='red', alpha=0.1)\n",
    "\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('SGLD Path in Latent Space')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Save the trained EBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoded = vae_biased.decode(paths[-1].to(device)).cpu().view(-1, 28, 28)\n",
    "\n",
    "fig, axes = plt.subplots(6, 6, figsize=(6, 6), gridspec_kw={'wspace': 0.05, 'hspace': 0.05})\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        axes[i, j].imshow(decoded[i * 6 + j], cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.suptitle('Generated Images from Energy-Weighted Prior Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the histogram of the resulting samples :\n",
    "class_decoded = model(decoded.view(-1, 28 * 28))\n",
    "class_decoded = class_decoded.argmax(dim=1).numpy()\n",
    "plt.hist(class_decoded, bins=np.arange(11) - 0.5, density=True)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Generated Samples from EBM')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon0403",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
