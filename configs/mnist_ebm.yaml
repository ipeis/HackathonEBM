
_target_: src.models.EBM
_recursive_: false  

train:
  batch_size: 128
  epochs: 300
  accelerator: "cuda"
  devices: 1
  precision:  16
  strategy: "auto"
  gradient_clip_val: 0.1
  optimizer:
    _target_: torch.optim.Adam
    lr: 1e-4
    weight_decay: 0.
    betas: [0., 0.999]
  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 1
    gamma: 0.97

model:  
  input_size: [28,28]
  input_dim: 1
  lambda_reg: 1e-1
  buffer_size: 8192


  energy_net:
    _target_: src.layers.ConvEncoder
    input_channels: ${model.input_dim}
    input_size: ${model.input_size}
    latent_dim: 1
    hidden_channels: [32, 64, 128, 128]
    kernel_sizes: [5, 3, 3, 3]
    strides: [2, 2, 2, 2]
    paddings: [4, 1, 1, 1]
    batch_norm: False
    activation: "swish"
    probabilistic: False

  sampler:
    _target_: src.samplers.SGLD
    img_shape: [1,28,28]
    sample_size: ${train.batch_size}
    steps: 60
    step_size: 4
    noise_std: 0.005


data:

  size: [28, 28]
  dim: 1
  root: "mnist/"

  # Shared dataset class params
  dataset:
    _target_: src.data.MyMNIST
    root: "${data.root}"
    download: True
    transform:
      _target_: torchvision.transforms.Compose
      transforms:
        - _target_: torchvision.transforms.ToTensor
        - _target_: torchvision.transforms.Normalize
          mean: [0.5]
          std: [0.5]
      
  # Specific split params
  train_dataset:
      train: True
  val_dataset:
      train: False

  dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: ${train.batch_size}
    num_workers: 4
    pin_memory: True


callbacks:
  image_samples: 
    _target_: src.callbacks.ImageSamples
    num_samples: 64
    steps: 256
    every_n_epochs: 1
  
  buffer_samples: 
    _target_: src.callbacks.BufferSamples
    num_samples: 64
    every_n_epochs: 1

  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: "epoch"
  




